Decision Tree
    Decision trees  are a type of supervised learning algorithm that is capable of 
    making decisions based on input features. Decision Trees recursively split the data based on 
    feature values to create a tree-like structure where each node represents a decision based on a specific feature

Nodes: In a Decision Tree, nodes represent decision points.
Edges: Edges connect nodes and represent the outcome of a decision.
Decision Nodes :Nodes in the tree where a decision is made based on a feature.
Leaf Nodes: Nodes at the bottom of the tree that do not split further. They represent the final decision or the predicted outcome.
Branch: The connections between nodes, indicating the possible outcomes based on the decisions made at decision nodes
Parent Nodes: Node in the tree structure that has one or more branches leading to other nodes
Child Node: The branches from a parent node led to child nodes. The number of child nodes is determined by the number of possible outcomes or conditions 
for the chosen feature. For binary decision trees, there are typically two child nodes.

CART (Classification and Regression Trees) 
is an algorithm used for constructing decision trees that can be applied to both classification 
and regression problems.

Classification: A common criterion is Entropy. 
Regression:  The algorithm minimizes the variance of the target variable in the resulting subsets.

Ensemble Learning
    Ensemble learning merges predictions from multiple models to enhance accuracy in forecasting.


Basic Techniques
    Max Voting
    Averaging
    Weighted Averaging

Advanced Techniques 
Boosting 
Boosting is an ensemble technique where a sequence of models are trained, 
and each subsequent model corrects the errors of the previous ones.

  AdaBoost
  GradientBoost
Bagging 
Bagging involves training multiple instances of the same model on different subsets of the 
training data. The predictions from each model are then averaged or voted upon to make the final 
prediction.
  Bootstrap Aggregation 
  RandomForest



AdaBoost, also known as Adaptive Boosting, is a Boosting ensemble technique. Initially, in 
AdaBoost, the data provided constructs a model and generates predictions using the initial 
model. Then, the error is evaluated by comparing the predicted values to the actual values, and 
higher weights are assigned to the instances that were incorrectly predicted. A subsequent model 
is then created to correct the errors made by the previous model, and predictions are generated 
using the updated model. This process is iterated, with weighted errors being assigned and the 
errors from the previous model being corrected. The final model is generated by assigning 
weights to the average of all the individual models. 

Gradient Boosting is a boosting technique that merges weak learners to create a stronger model. 
In each iteration, a new model is trained to reduce the loss function from the previous model by 
applying gradient descent. The algorithm calculates the gradient of the loss function based on the 
current ensembleâ€™s predictions. A new weak model is then trained to minimize this gradient, and 
its predictions are incorporated into the ensemble. This iterative process continues until a 
specified stopping condition is reached


The Random Forest algorithm begins by creating random subsets of the original dataset using 
bootstrapping. At each decision tree node, a random subset of features is used to determine the 
best split while introducing variability. A decision tree is independently trained on each subset, 
capturing different characteristics of the data. The final prediction is calculated by averaging the 
predictions from all decision trees which gives a robust and ensemble-based prediction. 
