Number of layers  
Number of neurons per layer 
Activation Function  (activation functions introduce non-linearity)
    ReLU 
    Sigmoid 
    Tanh 
    Softmax 
Learning rate 
Optimizers or Gradient Decent Algorithms 
    Stochastic Gradient Decent (SGD) 
    Adaptive Moment Estimation (Adam) 
    Root Mean Square Propagation (RMSprop) 
    Adaptive Gradient Algorithm (Adagrad) 
Batch Size  
Number of Epochs 
Cost Function  
    Mean Squared Error 
    Mean Absolute Error 
    Cross Entropy Cost Function  
    Mean Squared Logarithmic Error  
    Binary Cross Entropy  
    Categorical Cross Entropy 
Dropout Rate 

Epochs  
  An epoch is when the model goes through the entire training dataset once. During this time, it 
  updates its weights and biases based on the errors it makes. And it makes sure that the learning 
  process covers all data points at once. 

Batch size 
  Batch size is the number of samples processed together in one iteration during training. Smaller 
  batch sizes allow for more frequent updates and can help avoid overfitting, while larger batch 
  sizes speed up training but may require more memory. 

Optimizer 
  An optimizer is an algorithm used to adjust the weights and biases of a neural network to 
  minimize the cost function. 
  Optimizers – SGD, RMSprop, Adagrad, Adam

Activation function  
    Activation function introduces non-linearity into the neural network. It allows the neural network 
    models to learn complex patterns and relationships in data. It applies a mathematical function to 
    the input of each neuron to decide if the neuron should be activated or not.  
    Activation functions – ReLU, Sigmoid, Tanh, Softmax 

Cost function 
    A cost function measures the error between the predicted output of a neural network and the 
    actual target value. It gives a single value that represents how well the network is performing, 
    with smaller values indicating better performance. 
    Cost functions – Mean Squared Error, Mean Absolute Error, Cross Entropy Cost Function

Feedforward
     Feedforward is the process of passing the input data through the neural network to obtain the output prediction.
    The goal of feedforward is to make predictions based on the current weights of the neural network

Backpropagation
    Backpropagation (short for "backward propagation of errors") is the process used to update 
    the weights of the neural network to minimize the difference between the predicted output 
    and the actual target values.


